# Credit Card Fraud Detection — Big Data Pipeline

> An end-to-end demonstration of a real-time fraud detection system using Apache Spark, Kafka, Cassandra, and Flask.

![Architecture](architecture.png)

## Overview

This project implements a realistic big data pipeline for detecting credit card fraud. It handles the full lifecycle of data: ingestion, storage, batch model training, real-time streaming inference, and visualization.

**Key Technologies:**
* **ETL & Machine Learning:** Apache Spark (PySpark)
* **Streaming Ingress:** Apache Kafka
* **Storage:** Apache Cassandra (with Parquet fallback)
* **Visualization:** Flask + Socket.IO dashboard

## Project Structure

```text
├── src/
│   ├── spark_data_import.py       # ETL: Loads CSVs, transforms features, writes to Cassandra/Parquet
│   ├── spark_ml_training.py       # ML: Trains Random Forest model, generates metrics report
│   ├── spark_streaming_detector.py # Streaming: Kafka → Spark Streaming → ML Model → Cassandra
│   ├── kafka_producer.py          # Util: Sends synthetic transactions to Kafka for testing
│   ├── dashboard.py               # UI: Flask web app pushing real-time alerts via Socket.IO
│   ├── benchmark.py               # Test: Pandas vs. Spark ETL performance benchmark
│   └── kafka_load_test.py         # Test: High-throughput Kafka producer load test
├── dataset/                       # Place input CSVs here (fraudTrain.csv, fraudTest.csv)
├── models/                        # Serialized Spark models saved after training
├── reports/                       # Generated text reports (training metrics, benchmarks)
├── run.sh                         # Convenience wrapper script
├── requirements.txt               # Python dependencies
└── docker-compose.yml             # Infrastructure (Cassandra, Kafka, Zookeeper)
```

## Dataset & Methodology
**Source:** Credit Card Fraud Detection by Kartik2112 (Kaggle)

**Attribution:** Synthetic data generated by the Sparkov Data Generation tool (Brandon Harris).

**Data Details:**

* **Volume:** ~1.85 million transactions (2019–2020).
* **Features:** Transaction time, merchant info, amount, and customer demographics (job, location, etc.).

**Processing:**

* **ETL:** Parses timestamps, calculates geospatial distances (customer vs. merchant), and derives features like age and hour.
* **Training:** Uses a Spark ML pipeline (Indexer, VectorAssembler, Scaler, Random Forest Classifier).
* **Reporting:** The training script outputs a detailed report to `reports/` containing the confusion matrix, F1 score, and feature importances.

## Prerequisites
* **Docker & Docker Compose:** Required to run the infrastructure (Kafka, Zookeeper, Cassandra).
* **Python 3.10+:** Recommended to use a virtual environment.
* **Java (JDK 8 or 11):** Required if running Spark locally without a container.

## Quick Start
1. **Start Infrastructure**
   Launch the database and message broker services using Docker Compose.

```bash
docker compose up -d
```

2. **Configure Environment**
   Set up the Python virtual environment and install dependencies.

```bash
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

3. **Run ETL (Data Import)**
   Loads the CSV data, transforms it, and saves it to Cassandra (and Parquet for local fallback).

```bash
# Ensure fraudTrain.csv and fraudTest.csv are in the dataset/ folder
python src/spark_data_import.py
```

4. **Train Model**
   Trains the Random Forest model and saves it to models/. A detailed performance report is saved to reports/.

```bash
python src/spark_ml_training.py

# View the generated report
cat reports/spark_fraud_training_report_*.txt
```

5. **Start Streaming Pipeline**
   Start the Spark Structured Streaming job. This listens to Kafka, scores transactions using the saved model, and writes alerts to Cassandra.

```bash
python src/spark_streaming_detector.py
```

6. **Generate Traffic**
   In a separate terminal, start the producer to simulate live transaction traffic.

```bash
python src/kafka_producer.py
```

7. **Launch Dashboard**
   In a third terminal, start the web interface to view real-time alerts.

```bash
python src/dashboard.py
# Open http://localhost:8080 in your browser
```

## Benchmarking & Load Testing
**ETL Benchmark (Pandas vs Spark):** Compares processing time for large CSV ingestion.

```bash
python src/benchmark.py
# Results saved to reports/benchmark_*.json
```

**Kafka Load Test:** Tests the system under high throughput.

```bash
# Example: 100 messages/sec for 30 seconds
python src/kafka_load_test.py --rate 100 --duration 30
# Results saved to reports/loadtest_*.json
```

## Troubleshooting
**HDFS / File System Errors:**

The scripts are configured to use the local filesystem (`file:///`). If you see HDFS connection errors (e.g., connecting to `localhost:9000`), ensure `spark.hadoop.fs.defaultFS` is correctly set in the Spark session builder within the scripts.

**Cassandra Timeouts:**

Large `COUNT(*)` queries may time out on development clusters. The dashboard uses caching to mitigate this. If ETL writes time out, try reducing the batch size in `spark_data_import.py`.

**Kafka Topic Not Found:**

The streaming script attempts to create the `creditcardTransaction` topic automatically. If this fails, create it manually:

```bash
docker exec -it <kafka_container_name> kafka-topics --bootstrap-server localhost:
```

